# Empowering News Recommendation with Pre-trained Language Models

使用预训练的语言模型增强新闻推荐能力

## 摘要

​	个性化新闻推荐是在线新闻服务的一项重要技术。新闻文章通常包含丰富的文本内容，准确的新闻建模对于个性化新闻推荐很重要。现有的新闻推荐方法主要是基于传统的文本建模方法对新闻文本进行建模，对于挖掘新闻文本中的深层语义信息并不是最优的。预训练语言模型 (PLM) 对自然语言理解非常有用，它具有更好的新闻建模潜力。但是，没有公开报告显示 PLM 已应用于新闻推荐。在本文中，我们报告了我们利用预先训练的语言模型来增强新闻推荐的工作。单语和多语新闻推荐数据集的离线实验结果表明，利用 PLM 进行新闻建模可以有效提高新闻推荐的性能。我们基于 PLM 的新闻推荐模型已部署到 Microsoft 新闻平台，并在英语和全球市场的点击量和浏览量方面取得了显着提升。

## 1 引言

​	新闻推荐技术在许多在线新闻平台中发挥了关键作用，以减轻用户的信息过载[15]。新闻建模是新闻推荐的重要步骤，因为它是了解候选新闻内容的核心技术，也是从点击新闻推断用户兴趣的前提。由于新闻文章通常具有丰富的文本信息，因此新闻文本建模是理解新闻内容进行新闻推荐的关键。现有的新闻推荐方法通常基于传统的 NLP 模型 [15, 19, 20, 22, 23, 25, 26] 对新闻文本进行建模。例如，王等人。 [20] 提出使用知识感知的 CNN 网络从新闻标题中的单词和实体的嵌入中学习新闻表示。吴等人。 [23] 提出使用多头自注意力网络从新闻标题中学习新闻表示。然而，这些浅层模型很难理解新闻文本中的深层语义信息[18]。此外，他们的模型仅从新闻推荐任务中的监督中学习，这对于捕获语义信息可能不是最佳的。

​	预训练语言模型 (PLM) 凭借其强大的文本建模能力在 NLP 中取得了巨大成功 [2, 5, 6, 11, 12, 14, 28]。 与通常在特定任务中直接使用标记数据进行训练的传统模型不同，PLM 通常首先通过自我监督在大型未标记语料库上进行预训练，以编码通用文本信息 [5]。 因此，PLM 通常可以为下游任务的微调提供更好的初始点 [16]。 此外，与许多具有浅层模型的传统 NLP 方法不同 [9, 10, 24, 29]，PLM 通常更深，具有大量参数。 例如，BERT-Base 模型包含 12 个 Transformer 层和高达 109M 的参数 [5]。 因此，PLM 可能在对新闻文本中复杂的上下文信息进行建模方面具有更大的能力，这有可能改进用于新闻推荐的新闻文本建模。

​	在本文中，我们介绍了使用预训练语言模型增强大规模新闻推荐的工作。1 与使用浅 NLP 模型进行新闻建模的现有新闻推荐方法不同，我们探索使用预训练语言模型和 使用新闻推荐任务对它们进行微调。 真实世界英语和多语言新闻推荐数据集的离线实验验证，将 PLM 纳入新闻建模可以持续提高新闻推荐性能。 此外，我们的 PLM 支持的新闻推荐模型已部署到 Microsoft 新闻平台。2 据我们所知，这是第一次报道使用 PLM 为大型新闻推荐系统提供支持的努力。 在线飞行实验表明，我们基于 PLM 的新闻推荐模型在英语市场实现了 8.53% 的点击和 2.63% 的页面浏览量增长，在其他 43 个全球市场实现了 10.68% 的点击和 6.04% 的页面浏览量增长。

## 方法论

​	在本节中，我们将介绍 PLM 授权的新闻推荐的细节。 我们首先介绍通用的新闻推荐模型框架，然后介绍如何将 PLM 融入该框架以赋能新闻建模。

### 2.1 常见的新闻推荐框架

​	许多现有方法 [1, 15, 21, 23] 中使用的新闻推荐的一般框架如图 1 所示。该框架中的核心组件包括一个旨在从文本中学习新闻嵌入的新闻编码器，一个用户编码器从点击新闻的嵌入中学习用户嵌入，以及点击预测模块，根据用户嵌入和候选新闻嵌入之间的相关性计算新闻排名的个性化点击分数。我们假设用户有 𝑇 历史点击新闻，记为 $\left[D_{1}, D_{2}, \ldots, D_{T}\right]$。新闻编码器处理用户的这些点击新闻和每个候选新闻$D_c$以获得它们的嵌入，分别表示为$\left[\mathbf{h}_{1}, \mathbf{h}_{2}, \ldots, \mathbf{h}_{T}\right]$和$h_c$。它可以通过各种 NLP 模型来实现，例如 CNN [10] 和 self-attention [18]。用户编码器接收点击的新闻嵌入序列作为输入，并输出一个总结用户兴趣信息的用户嵌入 u。它也可以通过各种模型来实现，例如 [15] 中使用的 GRU 网络、[21] 中使用的注意力网络以及 [23] 中使用的多头自注意力和加性注意力网络的组合。点击预测模块将用户嵌入u和h𝑐作为输入，并通过评估它们的相关性来计算点击分数$\hat{y}$。它也可以通过内积[15]、神经网络[20]和分解机[7]等各种方法来实现。

### 2.2 PLM赋能新闻推荐

​	接下来，我们介绍 PLM 授权的新闻推荐框架，如图 2 所示。我们使用预训练的语言模型实例化新闻编码器，以捕获新闻文本中的深层上下文，并使用注意力网络来汇集 PLM 的输出。 我们将带有 𝑀 标记的输入新闻文本表示为$[w_{1}, w_{2}, \ldots, w_{M}]$。 PLM 将每个标记转换为其嵌入，然后通过几个 Transformer 层学习单词的隐藏表示。 我们将隐藏标记表示序列表示为$\left[\mathbf{r}_{1}, \mathbf{r}_{2}, \ldots, \mathbf{r}_{M}\right]$。 我们使用注意力 [29] 网络将隐藏的令牌表示总结为统一的新闻嵌入。 PLM 和注意力网络学习的新闻嵌入进一步用于用户建模和候选匹配。

### 2.3 模型训练

​	接下来[22,23]，我们还使用负采样技术从原始新闻印象日志中构建标记样本，并通过对被点击的候选新闻进行分类，使用交叉熵损失函数进行模型训练。通过反向传播优化损失函数，可以针对新闻推荐任务调整推荐模型和plm中的参数。

## 3 实验

### 3.1 数据集和实验设置

​	我们的离线实验是在两个真实世界的数据集上进行的。 第一个是 MIND [27]，这是一个用于单语新闻推荐的英文数据集。 它包含了 6 周内 100 万用户在 Microsoft News 上的新闻点击日志。3 第二个是我们在 MSN 新闻平台上于 2020 年 12 月 1 日至 1 月 14 日收集的多语种新闻推荐数据集（记为 Multilingual）， 2021.它包含来自7个不同语言使用国家的用户，他们的市场语言代码分别是EN-US、DE-DE、FR-FR、IT-IT、JA-JP、ES-ES和KO-KR。 我们在每个市场随机抽取 200,000 条印象日志。 上周的日志用于测试，其余用于训练和验证（9:1 拆分）。 两个数据集的详细统计数据如表1所示。

​	在我们的实验中，如果没有特别提及，我们使用了不同预训练语言模型的“基础”版本。 我们微调了最后两个 Transformer 层，因为我们发现微调所有层和最后两层之间只有非常小的性能差异。 在[27]之后，我们使用新闻的标题进行新闻建模。 我们使用 Adam [3] 作为优化算法，学习率为 1e-5。 批量大小为 128.4 这些超参数是在验证集上开发的。 我们使用所有展示次数的平均 AUC、MRR、nDCG@5 和 nDCG@10 作为性能指标。 我们独立地重复每个实验 5 次并报告平均性能。

### 3.2 离线性能评估

​	我们首先在 MIND 数据集上比较了几种方法的性能，以验证基于 PLM 的模型在单语新闻推荐中的有效性。我们比较了几种最近的新闻推荐方法，包括 EBNR [15]、NAML [21]、NPA [22]、LSTUR [1]、NRMS [23] 及其由不同预训练语言模型（包括 BERT [5] ）授权的变体，罗伯塔 [14] 和 UniLM [2]。结果如表 2 所示。参考该表，我们发现结合预训练的语言模型可以持续提高基本模型的性能。5 这是因为预训练的语言模型比学习的浅层模型具有更强的文本建模能力从头开始在新闻推荐中。此外，我们发现基于 RoBERTa 的模型优于基于 BERT 的模型。这可能是因为 RoBERTa 的超参数设置比 BERT 更好，并且在更大的语料库上进行了更长时间的预训练。此外，基于 UniLM 的模型实现了最佳性能。这可能是因为 UniLM 可以在文本理解和生成任务中利用自我监督信息，这有助于学习更高质量的 PLM。

​	此外，我们在多语言数据集上进行了实验，以验证 PLM 在多语言新闻推荐中的有效性。我们比较了 EBNR、NAML、NPA、LSTUR 和 NRMS 与不同多语言文本建模方法的性能，包括：（1）MUSE [13]，使用模块化无监督语义嵌入； (2) Unicoder [8]，一种由跨语言自监督任务预训练的通用语言编码器； (3) InfoXLM [4]，一种基于信息论框架的对比预训练跨语言语言模型。在这些方法中，按照[8]，我们混合了不同语言的训练数据。此外，我们还比较了基于 MUSE 的独立学习单语模型在每个市场（表示为 Single）的性能。不同方法在 AUC 方面的结果如表 3 所示。我们发现多语言模型通常优于独立学习的单语言模型。这可能是因为不同的语言通常有一些内在的相关性，不同国家的用户也可能有一些相似的兴趣。因此，使用多语言数据联合训练模型可以帮助学习更准确的推荐模型。它还提供了使用统一推荐模型为使用不同语言（例如印欧语和阿尔泰语）的不同国家/地区的用户提供服务的潜力，这可以大大降低在线服务的计算和内存成本。此外，基于多语言 PLM 的性能方法优于基于 MUSE 嵌入的性能方法。这可能是因为 PLM 在捕获复杂的多语言语义信息方面也比词嵌入更强大。此外，InfoXLM 能够比 Unicoder 更好地赋能多语种新闻推荐。这可能是因为 InfoXLM 使用比 Unicoder 更好的对比预训练策略来帮助学习更准确的模型。

### 3.3 模型大小的影响

​	接下来，我们探讨 PLM 大小对推荐性能的影响。 我们比较了两种代表性方法（即 NAML 和 NRMS）与不同版本 BERT 的性能，包括 BERT-Base（12 层）、BERT-Medium（8 层）、BERTSmall（4 层）和 BERT-Tiny（2 层） ）。 MIND 上的结果如图 3 所示。我们发现使用具有更多参数的更大 PLM 通常会产生更好的推荐性能。 这可能是因为较大的 PLM 通常具有更强的捕捉新闻深层语义信息的能力，如果结合更多的巨型 PLM（例如 BERT-Large），性能可能会进一步提高。 但是，由于大型 PLM 对于在线应用程序来说过于繁琐，我们更喜欢 PLM 的基本版本。

### 3.4 不同池化方法的影响

​	我们还探索使用不同的池化方法从 PLM 的隐藏状态中学习新闻嵌入。 我们比较了三种方法，包括：（1）CLS，使用“[CLS]”token的表示作为新闻嵌入，这是一种广泛使用的获取句子嵌入的方法； （2）Average，使用PLM隐藏状态的平均值； (3) 注意力，使用注意力网络从隐藏状态中学习新闻嵌入。 NAML-BERT 和 NRMS-BERT 在 MIND 上的结果如图 4.6 所示。我们发现 CLS 方法的性能最差是非常有趣的。 这可能是因为它无法利用 PLM 的所有输出隐藏状态。 此外，注意力优于平均。 这可能是因为注意力网络可以区分隐藏状态的信息量，这有助于学习更准确的新闻表示。 因此，我们选择注意力机制作为池化方法。

### 3.5 嵌入向量可视化

​	我们还研究了浅层模型和 PLM 授权模型学习的新闻嵌入之间的差异。我们使用 t-SNE [17] 来可视化 NRMS 和 NRMSUniLM 学习的新闻嵌入，结果如图 5 所示。我们发现一个有趣的现象是，NRMS-UniLM 学习的新闻嵌入比 NRMS 更具辨别力。这可能是因为 NRMS 中的浅层自注意力网络无法有效地对新闻文本中的语义信息进行建模。由于用户兴趣也是从点击新闻的嵌入中推断出来的，因此 NRMS 很难从非歧视性新闻表示中准确地建模用户兴趣。此外，我们观察到 NRMS-UniLM 学习到的新闻嵌入形成了几个清晰的集群。这可能是因为 PLM 授权的模型可以解开不同类型的新闻，以实现更好的用户兴趣建模和新闻匹配。这些结果表明，深度 PLM 比浅层 NLP 模型在学习判别性文本表示方面具有更大的能力，这通常有利于准确的新闻推荐。

### 3.6 在线飞行实验

​	我们已将我们的 PLM 授权新闻推荐模型部署到 Microsoft 新闻平台中。 我们的 NAML-UniLM 模型用于为英语市场的用户提供服务，包括 EN-US、EN-GB、EN-AU、EN-CA 和 EN-IN。 在线飞行实验结果显示，与之前没有预训练语言模型的新闻推荐模型相比，点击量增加了 8.53%，浏览量增加了 2.63%。 此外，我们的 NAML-InfoXLM 模型被用于为其他 43 个市场的用户提供不同语言的服务。 在线航班结果显示点击量提高了 10.68%，网页浏览量提高了 6.04%。 这些结果验证了将预训练的语言模型结合到新闻推荐中可以有效地提高在线新闻服务的推荐性能和用户体验。

## 4 结论

​	在本文中，我们介绍了使用预训练语言模型增强个性化新闻推荐的工作。 我们对英语和多语言新闻推荐数据集进行了广泛的离线实验，结果表明结合预训练的语言模型可以有效地改进新闻推荐的新闻建模。 此外，我们基于 PLM 的新闻推荐模型已部署到商业新闻平台，这是首次公开报道的使用 PLM 为现实世界大规模新闻推荐系统提供支持的努力。 在线航班结果显示，在使用不同语言的大量市场中，点击量和网页浏览量都有显着改善。



























