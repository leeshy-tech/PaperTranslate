# Personalized News Recommendation with Knowledge-aware Interactive Matching

基于知识感知交互匹配的个性化新闻推荐

## 摘要

​	个性化新闻推荐最重要的任务是准确匹配候选新闻和用户兴趣。大多数现有的新闻推荐方法都以独立的方式根据其文本内容和用户点击新闻的用户兴趣对候选新闻进行建模。然而，一篇新闻文章可能涵盖多个方面和实体，并且用户通常具有不同类型的兴趣。候选新闻和用户兴趣的独立建模可能导致新闻和用户之间的匹配较差。在本文中，我们提出了一种用于新闻推荐的知识感知交互式匹配方法。我们的方法以交互方式对候选新闻和用户兴趣进行建模，以促进它们的准确匹配。我们设计了一个知识感知新闻协同编码器，在知识图谱的帮助下捕获它们在语义和实体中的相关性，交互式地学习点击新闻和候选新闻的表示。我们还设计了一个用户新闻协同编码器来学习候选新闻感知用户兴趣表示和用户感知候选新闻表示，以实现更好的兴趣匹配。在两个真实世界数据集上的实验验证了，我们的方法可以有效地提高新闻推荐的性能。

## 1 引言

​	Microsoft News、Apple News 和 News Break 等在线新闻平台吸引了大量用户消费新闻信息 [21, 33]。 然而，由于这些平台每天都会收集大量新发布的新闻文章，用户往往很难找到他们需要的新闻信息[34、47]。 个性化新闻推荐技术旨在帮助用户找到他们感兴趣的新闻，通常在在线新闻平台中发挥重要作用，以缓解用户的信息过载[1, 32]。 因此，个性化新闻推荐的研究引起了学术界和工业界的广泛关注[1,2,10,13,19,42,43,46]。

​	用户兴趣和候选新闻之间的准确匹配对于个性化新闻推荐至关重要 [31, 32]。 现有方法通常根据其文本信息对候选新闻进行建模，并以独立的方式从用户的点击历史中推断出用户兴趣[21, 37]。 例如，吴等人。 [34] 通过单词级个性化注意力网络学习新闻表示，并通过用户级个性化注意力网络学习用户兴趣表示。 他们进一步通过用户兴趣表示和候选新闻表示的内积进行兴趣匹配。 然而，候选新闻文章可能包含多个方面和实体 [18、33]，并且用户可能有多个兴趣 [32]。 因此，候选新闻和用户兴趣的独立建模可能不如兴趣匹配[31]。

​	在本文中，我们探索更好地模拟候选新闻和用户兴趣之间的相关性，以实现准确的兴趣匹配。 我们的论文受到以下观察的启发。 首先，候选新闻可能涵盖不同的方面和实体，并且用户可能有多种兴趣。 例如，图 1 中的第 2 条候选新闻与篮球明星和政治家相关，涵盖多个实体，例如“斯蒂芬·库里”和“唐纳德·特朗普”。 此外，图 1 中的示例用户对汽车、音乐和体育等多个领域感兴趣。 第二个候选新闻只能匹配特定的用户兴趣，即体育，并且用户可能只对第二个候选新闻中的单个实体感兴趣，即“斯蒂芬库里”。因此，如果它们是独立建模的，用候选新闻匹配用户兴趣的效果是较差的。其次，候选新闻和点击新闻的语义匹配有助于更准确地进行兴趣匹配。例如，第 2 条点击新闻也与第 1 条候选新闻具有语义相关性，因为它们都与音乐有关。第 3 条点击的新闻与第 1 条候选新闻具有语义相关性，因为它们提到了相同的事件。基于这些语义相关性，我们可以推断用户可能对第一个候选新闻感兴趣。第三，借助知识图谱，点击新闻和候选新闻中实体之间的知识匹配也有助于了解用户对候选新闻的兴趣。例如，第 4 条点击新闻中的实体“史蒂夫·科尔”与第 2 条候选新闻中的实体“斯蒂芬·库里”具有内在关联，因为前者和后者分别是“NBA”勇士队的球员和教练。根据知识匹配，我们可以推断出用户可能对第二个候选新闻感兴趣。因此，在语义和知识层面利用点击新闻和候选新闻之间的相关性有利于兴趣匹配。

​	在本文中，我们提出了一种用于个性化新闻推荐的知识感知交互式匹配框架（命名为KIM）。我们的方法可以交互地对候选新闻和用户兴趣进行建模，以学习候选新闻感知的用户兴趣表示和用户感知的候选新闻表示，从而更准确地匹配用户兴趣和候选新闻。在该框架中，我们提出了一种知识协同编码器，借助知识图谱从点击新闻中的实体与候选新闻中的实体之间的相关性来建模用户对候选新闻的兴趣。更具体地说，我们首先提出了一个图协同注意网络，它通过选择和聚合对兴趣匹配有用的邻居，从知识图谱中学习实体的表示。我们进一步提出了使用实体协同注意网络，它通过捕获实体之间的相关性来交互式地学习点击新闻和候选新闻的基于知识的表示。此外，我们还提出了一种语义协同编码器，通过对文本之间的语义相关性进行建模，交互式地学习用户点击新闻和候选新闻的基于语义的表示。新闻的统一表示被表述为基于知识和语义的表示的聚合。此外，我们进一步提出了一种用户新闻联合编码器，用于从点击新闻和候选新闻的表示中构建候选新闻感知的用户兴趣表示和用户感知的候选新闻表示，以更好地模拟用户对候选新闻的兴趣。最后，根据候选新闻的表示与用户兴趣之间的相关性对候选新闻进行排名。我们对两个真实世界的数据集进行了广泛的实验，并表明我们的方法可以有效地提高新闻推荐的性能并优于其他基线方法。

## 2 相关的工作

​	个性化新闻推荐是在线新闻服务的一项重要任务[4, 17]，近年来得到了广泛的研究[14,16,24,30,35,36,41,44]。 现有方法通常根据其内容对候选新闻进行建模，并根据点击新闻独立建模用户兴趣，然后根据候选新闻和用户兴趣的相关性进行匹配[5,32,33,37,38]。 例如，Okura 等人。 [21] 通过自动编码器表示来自其主体的候选新闻，并通过 GRU 网络独立地表示来自用户点击历史的用户兴趣。他们根据他们的表示之间的点积进一步匹配用户兴趣和候选新闻。吴等人。 [37] 采用多头自注意力网络从标题中对候选新闻进行建模，并采用另一个多头自注意力网络从用户的点击历史中建模用户兴趣。刘等人。 [18]提出从新闻标题中的实体及其知识图谱上的邻居学习基于知识的候选新闻表示，并通过注意力网络从用户点击的新闻中学习用户兴趣表示。此外，这些方法还通过用户兴趣表示和候选新闻表示的内积进行兴趣匹配。一般来说，候选新闻可能涵盖多个方面和实体 [18, 33]，并且用户可能有多个兴趣 [32]。只有一部分候选新方面和用户兴趣对匹配用户兴趣和候选新闻有用。然而，这些方法独立地对候选新闻和用户兴趣进行建模，这对于进一步的兴趣匹配可能较差。与这些方法不同的是，在 KIM 方法中，我们提出了一个知识感知的交互式匹配框架，在考虑相关性的情况下对候选新闻和用户兴趣进行交互建模，可以更好地将用户兴趣与候选新闻进行匹配。

​	一些方法以候选感知的方式模拟用户兴趣[32、47]。例如，王等人。 [32] 通过多通道 CNN 网络从新闻标题中对齐的单词和实体中获取新闻表示。此外，他们应用了一个候选感知注意力网络，通过根据与候选新闻的相关性聚合点击的新闻来学习用户兴趣表示。他们进一步使用密集网络从他们的表示中模拟用户兴趣和候选新闻的相关性。朱等人。 [47] 提出通过多个 CNN 网络从新闻标题中的单词和实体中学习新闻表示，并通过 LSTM 网络和候选感知注意力网络从历史点击中学习用户兴趣表示。他们根据表示的余弦相似度匹配用户兴趣和候选新闻。事实上，候选新闻可能包含多个方面和实体[18、33]，其中只有一部分可能与用户兴趣相匹配。然而，这些方法在没有考虑目标用户的情况下对候选新闻进行建模，这对于进一步将用户兴趣与候选新闻进行匹配可能较差。与这些方法不同，我们的 KIM 方法在考虑目标用户的情况下对候选新闻进行建模。此外，这些方法在没有考虑相关性的情况下对点击新闻和候选新闻进行建模，这对于进一步衡量候选新闻和从点击新闻推断出的用户兴趣之间的相关性也可能不是最优的。与这些方法不同，KIM 可以交互地学习点击新闻和候选新闻的表示，以实现更好的兴趣匹配。

## 3 方法论

​	我们首先介绍个性化新闻推荐的问题定义。 接下来，我们详细介绍我们的用于个性化新闻推荐的知识感知交互式匹配框架（命名为 KIM）。

### 3.1 问题表述

​	给定一个用户𝑢和一个候选新闻$$n^c$$，我们需要计算相关性分数𝑧来衡量用户𝑢对候选新闻内容$$n^c$$的兴趣。 然后根据相关性得分对不同的候选新闻进行排名并推荐给用户𝑢。 用户𝑢与他/她点击的新闻集相关联。 每个新闻 𝑛 都与其文本 𝑇 和文本中的实体 𝐸 相关联。 此外，还有一个知识图 G 用于提供实体之间的相关性。 它包含实体和实体之间的关系。 G 中的每个实体 𝑒 与其嵌入相关联，e 基于知识图进行预训练。 在我们的方法中，我们只使用实体之间的链接来表示它们的相关性，而不使用特定的关系（例如，located_at）。

### 3.2 KIM的框架

​	在本节中，我们将介绍 KIM 的新闻推荐框架，该框架可以交互地对候选新闻和用户兴趣进行建模，以实现更好的兴趣匹配。 如图 2 所示，KIM 包含两个主要模块。 第一个是知识感知新闻协同编码器，它通过捕获语义和知识层面的相关性，交互式地学习用户点击新闻和候选新闻的知识感知表示。 第二个是用户新闻协同编码器，它从用户点击新闻的表征和知识感知新闻协同编码器生成的候选新闻表征中交互学习候选新闻感知用户兴趣表征u和用户感知候选新闻表征c . 最后，我们根据候选新闻感知用户兴趣表示和用户感知候选新闻表示之间的相关性，将候选新闻与用户兴趣匹配。 接下来，我们详细介绍各个模块。

### 3.3 知识感知新闻协同编码器（knowledge-aware news co-encoder）

​	在本节中，我们介绍了知识感知新闻协同编码器的框架，它从用户的文本和文本中的实体中交互式地学习用户点击的新闻$$n_u$$和候选新闻$$n_c$$的表示。 如图 3 所示，它包含三个子模块。 第一个是知识协同编码器（记为 $$\Phi_{k}$$），对于点击新闻$$n_u$$和候选新闻$$n_c$$，它基于知识图谱从实体之间的相关性中交互式地学习基于知识的表示$$\mathbf{k}^{u} \in R^{d_{k}}$$和 $$\mathbf{k}^{c} \in \mathcal{R}^{d_{k}}$$：
$$
\left[\mathbf{k}^{u}, \mathbf{k}^{c}\right]=\Phi_{k}\left(E^{u}, E^{c}\right)\tag 1
$$
​	其中$$d_k$$表示基于知识的新闻表示维度，$$E^u$$和$$E^c$$分别表示新闻$$n^u$$和$$n^c$$中的实体。 第二个是语义协同编码器（表示为 $$\Phi_{t}$$），它交互式地学习基于语义的表示$$ \mathbf{t}^{u} \in R^{d_{t}}$$和$$ \mathbf{t}^{c} \in R^{d_{t}}$$，用于新闻$$n_u$$和$$n_c$$，以根据文本之间的语义相关性来模拟用户对候选新闻的兴趣：
$$
\left[\mathbf{t}^{u}, \mathbf{t}^{c}\right]=\Phi_{t}\left(T^{u}, T^{c}\right)\tag 2
$$
​	其中$$d_t$$表示基于语义的新闻表示维度，$$T^u$$和$$T^u$$分别表示新闻$$n_u$$和$$n_c$$的文本。 最后，我们投影同一个新闻的基于知识和语义的表示来学习统一的新闻表示：
$$
\mathbf{n}^{u}=\mathbf{P}_{n}\left[\mathbf{t}^{u} ; \mathbf{k}^{u}\right], \quad \mathbf{n}^{c}=\mathbf{P}_{n}\left[\mathbf{t}^{c} ; \mathbf{k}^{c}\right]\tag 3
$$
​	其中$$\mathbf{n}^{u} \in R^{d_{n}}$$表示用户点击新闻$$n_u$$的知识感知表示，$$\mathbf{n}^{c} \in R^{d_{n}}$$表示候选新闻的相应知识感知表示$$n_c$$ ，$$d_n$$是新闻表示维度，[·; ·] 表示连接操作，$$\mathbf{P}_{n} \in R^{d_{n} \times\left(d_{t}+d_{k}\right)}$$是可训练的投影矩阵。

#### 3.3.1 知识协同编码器

​	我们介绍了所提出的知识协同编码器，它交互式地学习用户点击新闻$$n_u$$和候选新闻$$n_c$$的基于知识的表示。 它旨在借助知识图谱G更好地表示这些新闻，以根据用户点击新闻和候选新闻中实体$$E^u$$和$$E^c$$之间的相关性进行兴趣匹配。如图4所示，它包含三个组件。 首先，为了总结$$E^u$$或$$E^c$$中每个实体在 𝐾 跳内的邻居的信息，我们首先利用图注意 (GAT) 网络 [28] 堆叠 𝐾 层来学习它们的表示，分别表示为$$\mathbf{M}_{u}=\left\{\mathbf{m}_{i}^{u}\right\}_{i=1}^{D} \in \mathcal{R}^{d_{k} \times D} \text { and } \mathbf{M}_{c}=\left\{\mathbf{m}_{i}^{c}\right\}_{i=1}^{D} \in \mathcal{R}^{d_{k} \times D}$$，其中𝐷是新闻中实体的数量。

​	第二个是本文提出的堆叠图协同注意（GCAT）网络。 一般来说，一个实体通常与知识图谱上的多个实体具有丰富的相关性 [6, 29]。

​	此外，实体之间的相关性通常提供不同的信息量来模拟点击新闻和候选新闻之间的相关性以进行兴趣匹配。例如，给定一条点击的新闻“Style is the trending song today”。和一个候选新闻“Movie Cats是 Netflix 中最受欢迎的电影。”，实体“Movie Cats”在知识图谱上有许多邻居实体，例如它的导演“James”、首席演员“Hooper”、首席女演员“Taylor”等。只有实体“Taylor”对点击新闻和候选新闻之间的相关性建模具有信息性，因为它也是点击新闻中实体“Song Style”的歌手。为了更好地选择实体之间的信息相关性以将候选新闻与用户兴趣相匹配，我们提出了一个图共同注意网络（GCAT）堆叠的𝐾层来学习新闻中实体的匹配感知表示$$n^u$$和$$n^c$$。以新闻$$n^u$$中的实体 𝑒 为例，图 5 所示的第 𝑙 图共同注意网络通过聚合新闻中的实体 $$n^c$$引导的邻居的表示来学习其表示。更具体地说，我们首先将多头自注意力网络 [27] 应用于邻居实体的表示，它是由第(𝑙 - 1)GCAT 网络生成的，以模拟不同邻居实体之间的概念相关性。接下来，我们提出用一个匹配感知注意网络来聚合实体 𝑒 的邻居实体，基于它们与新闻$$n^c$$中的实体的相关性，由相关矩阵$$\mathbf{I}_{u} \in \mathcal{R}^{D \times B}$$测量：
$$
\mathbf{I}_{u}=\mathbf{M}_{c}^{T} \mathbf{W}_{c}^{c} \hat{\mathbf{G}}_{l}\tag 4
$$
​	其中$$\hat{\mathbf{G}}_{l}=\left\{\hat{\mathrm{g}}_{i}^{l}\right\}_{i=1}^{B} \in \mathcal{R}^{d_{k} \times B}$$表示自注意网络生成的邻居实体的表示，𝐵 表示邻居的数量，$$\mathbf{W}_{c}^{c} \in R^{d_{k} \times d_{k}}$$是可训练的权重。 然后相邻实体的注意力向量$$\mathbf{v}^{u} \in \mathcal{R}^{B}$$计算为：
$$
\mathbf{v}^{u}=\mathbf{q}_{e}^{T} \cdot \tanh \left(\mathbf{W}_{s}^{c} \hat{\mathbf{G}}^{l}+\mathbf{W}_{h}^{c} \mathbf{M}_{c} f\left(\mathbf{I}_{u}\right)\right)\tag 5
$$
​	其中𝑓表示对输入矩阵的每个列向量进行归一化的softmax激活，$$\mathbf{q}_{e} \in \mathcal{R}^{d_{q}}$$表示可训练的注意力查询，$$d_q$$表示其维度，$$\mathbf{W}_{s}^{c} \in \mathcal{R}^{d_{q} \times d_{k}}$$和$$\mathbf{W}_{h}^{c} \in \mathcal{R}^{d_{q} \times d_{k}}$$是可训练的权重。 然后我们将实体𝑒的邻居聚合成一个统一的表示$$\hat{\mathbf{g}}^{l} \in \mathcal{R}^{d_{k}}$$：
$$
\hat{\mathbf{g}}^{l}=\sum_{i=1}^{B} \lambda_{i}^{u} \hat{\mathbf{g}}_{i}^{l}, \quad \lambda_{i}^{u}=\frac{\exp \left(v_{i}^{u}\right)}{\sum_{j=1}^{B} \exp \left(v_{j}^{u}\right)}\tag 6
$$
​	其中$$v_i^u$$是向量$$v^u$$的第𝑖元素，$$\lambda_{i}^{u}$$表示第𝑖邻居实体的注意力权重。最后，第 𝑙 GCAT 网络生成的实体 𝑒 的表示$$\mathrm{g}^{l} \in \mathcal{R}^{d_{k}}$$表示为：$$\mathbf{g}^{l}=P_{e}\left[\hat{\mathbf{g}}^{l} ; \mathbf{g}^{\mathbf{l}-1}\right]$$，其中$$\mathbf{P}_{e} \in \mathcal{R}^{d_{k} \times 2 d_{k}}$$ 是投影矩阵。对于用户点击新闻中的实体，通过捕获他们在𝐾跳内的邻居和候选实体之间的相关性，堆叠的 GCAT 网络可以学习匹配感知表示$$\mathbf{s}_{u}=\left\{\mathbf{s}_{i}^{u}\right\}_{i=1}^{D} \in R^{d_{k} \times D}$$，其中$$s_i^u$$是点击新闻$$n^u$$中第𝑖个实体的表示。以一种对称的方式，我们可以从他们的邻居和点击新闻中的实体之间的相关性学习候选新闻中实体的匹配感知表示$$\mathbf{S}_{c}=\left\{\mathbf{s}_{i}^{c}\right\}_{i=1}^{D} \in R^{d_{k} \times D}$$，其中$$c^c_i$$是候选新闻$$n^c$$中的第i个实体。

​	第三个是实体共同注意网络。 点击新闻和候选新闻中的实体通常具有不同的兴趣匹配信息量。 例如，给定一条点击新闻“Style 是本周 iTunes 中的热门歌曲”。 和候选新闻“movie Cats是 Netflix 中最受欢迎的电影。”，实体“Song Style”比实体“iTunes”更能将用户兴趣与候选新闻匹配，因为实体“Song Style”候选新闻中的实体“movie Cats”具有固有的相关性。 因此，通过捕获实体之间的相关性，我们应用实体共同注意网络来交互式地学习新闻$$n^u$$和$$n^c$$的基于知识的表示。 详细地说，我们首先计算一个亲和矩阵$$\mathbf{C}_{e} \in \mathcal{R}^{D \times D}$$来衡量新闻中实体之间的相关性$$n^u$$和$$n^c$$：
$$
\mathbf{C}_{e}=\mathbf{S}_{c}^{T} \mathbf{W}_{c}^{k} \mathbf{S}_{u}\tag 7
$$
​	其中$$\mathbf{W}_{c}^{k} \in \mathcal{R}^{d_{k} \times d_{k}}$$是可训练的权重。 然后我们计算新闻$$n^u$$和$$n^c$$中实体的注意力向量$$a^u$$，$$\mathbf{a}^{c} \in \mathcal{R}^{D}$$：
$$
\mathbf{a}^{u}=\mathbf{q}_{k}^{T} \cdot \tanh \left(\mathbf{W}_{s}^{k} \mathbf{S}_{u}+\mathbf{W}_{h}^{k} \mathbf{S}_{c} f\left(\mathbf{C}_{e}\right)\right)\tag 8
$$

$$
\mathbf{a}^{c}=\mathbf{q}_{k}^{T} \cdot \tanh \left(\mathbf{W}_{s}^{k} \mathbf{S}_{c}+\mathbf{W}_{h}^{k} \mathbf{S}_{u} f\left(\mathbf{C}_{e}^{T}\right)\right)\tag 9
$$

​	其中$$\mathbf{q}_{k} \in \mathcal{R}^{d_{q}}$$是可训练的注意力查询，$$\mathbf{W}_{s}^{k} \in \mathcal{R}^{d_{q} \times d_{k}},\mathbf{W}_{h}^{k} \in \mathcal{R}^{d_{q} \times d_{k}}$$是可训练的权重。 最后，我们通过分别聚合实体获得点击新闻和候选新闻的基于知识的表示$$\mathbf{k}^{u} \in \mathcal{R}^{d_{k}} $$和$$ \mathbf{k}^{c} \in \mathcal{R}^{d_{k}}$$：
$$
\mathbf{k}^{u} &=\sum_{i=1}^{D} \alpha_{i}^{u} \mathbf{s}_{i}^{u}, & \alpha_{i}^{u} &=\frac{\exp \left(a_{i}^{u}\right)}{\sum_{j=1}^{D} \exp \left(a_{j}^{u}\right)}\\ \tag {10}
$$

$$
\mathbf{k}^{c} &=\sum_{i=1}^{D} \alpha_{i}^{c} \mathbf{s}_{i}^{c}, & \alpha_{i}^{c} &=\frac{\exp \left(a_{i}^{c}\right)}{\sum_{j=1}^{D} \exp \left(a_{j}^{c}\right)}\\ \tag{11}
$$

其中$$a_i^u$$和$$a_i^c$$分别表示新闻$$n^u$$和$$n^c$$中第𝑖个实体的注意力权重。

#### 3.3.2语义协同编码器

​	如图 4 所示，语义协编码器交互式地学习用户点击新闻$$n^u$$和候选新闻$$n^c$$的基于语义的表示。它旨在通过文本之间的语义相关性（$$T^u$$和$$T^c$$）更好地模拟用户对候选新闻的兴趣。我们首先独立学习文本$$T^u$$和$$T^c$$中单词的上下文表示。更具体地说，以文本$$T^u$$为例，我们首先通过词嵌入层将其转换为嵌入向量序列$$\mathbf{T}_{u} \in \mathcal{R}^{d_{g} \times M}$$，其中$$d_g$$表示词嵌入维度，𝑀表示$$T^u$$中的词数。接下来，由于局部和全局上下文对于语义建模 [33, 37] 都很重要，我们将 CNN 网络 [11] 和transformer网络 [27] 应用于$$T^u$$分别学习局部和全局上下文词表示，即$$\mathbf{L}_{u} \in \mathcal{R}^{d_{t} \times M} $$和$$\mathbf{J}_{u} \in \mathcal{R}^{d_{t} \times M}$$。然后，我们添加每个单词的局部和全局上下文表示，并获得它们的统一表示 $$\mathbf{H}_{u}=\left\{\mathbf{h}_{i}^{u}\right\}_{i=1}^{M} \in \mathcal{R}^{d_{t} \times M}$$，这里$$\mathbf{h}_{i}^{u} \in \mathcal{R}^{d_{t}}$$是$$T^u$$中第 𝑖 个词的表示。此外，对于$$T^c$$，我们可以以相同的方式学习上下文词表示$$\mathbf{H}_{c}=\left\{\mathbf{h}_{i}^{c}\right\}_{i=1}^{M} \in R^{d_{t} \times M}$$，其中$$\mathbf{h}_{i}^{c} \in \mathcal{R}^{d_{t}}$$是$$T^c$$中的第 𝑖 个词表示。

​	最后，一般来说，点击新闻和候选新闻中的不同语义方面通常对于匹配用户兴趣与候选新闻[39]具有不同的重要性。例如，给定一条被点击的新闻“苹果计划生产头戴式耳机”，它包含两个语义方面，即“苹果的产品计划”和“耳机”。前者对于将用户兴趣与候选新闻“2020 年最佳耳机”相匹配很重要。因为对耳机感兴趣的用户可能会同时点击它们。而后者对于将用户兴趣与候选新闻“iPhone 12 案例买家指南”相匹配很重要。因为对Apple产品感兴趣的用户可能会阅读它们。因此，我们应用语义共同注意网络 [25, 40] 通过捕获文本之间的语义相关性以进行兴趣匹配，以交互方式学习新闻$$n^u$$和$$n^c$$的基于语义的表示。具体来说，我们首先计算亲和矩阵$$\mathrm{C}_{t} \in \mathcal{R}^{M \times M}$$测量文本中不同单词之间的语义相关性$$T^u$$和$$T^c$$：
$$
\mathbf{C}_{t}=\mathbf{H}_{c}^{T} \mathbf{W}_{c}^{t} \mathbf{H}_{u}\tag {12}
$$
​	其中$$\mathbf{W}_{c}^{t} \in \mathcal{R}^{d_{t} \times d_{t}}$$是可训练的权重。 然后我们根据$$C_t$$分别计算用户点击新闻和候选新闻中单词的注意力向量$$\mathbf{b}^{u} \in \mathcal{R}^{M} $$和$$\mathbf{b}^{c} \in \mathcal{R}^{M}$$：
$$
\mathbf{b}^{u} &=\mathbf{q}_{t}^{T} \cdot \tanh \left(\mathbf{W}_{s}^{t} \mathbf{H}_{u}+\mathbf{W}_{h}^{t} \mathbf{H}_{c} f\left(\mathbf{C}_{t}\right)\right) \\ \tag {13}
$$

$$
\mathbf{b}^{c} &=\mathbf{q}_{t}^{T} \cdot \tanh \left(\mathbf{W}_{s}^{t} \mathbf{H}_{c}+\mathbf{W}_{h}^{t} \mathbf{H}_{u} f\left(\mathbf{C}_{t}^{T}\right)\right)\\ \tag{14}
$$

​	其中$$\mathbf{q}_{t} \in \mathcal{R}^{d_{q}}$$是可训练的注意力查询，$$\mathbf{W}_{s}^{t} \in \mathcal{R}^{d_{q} \times d_{l}}$$和$$\mathbf{W}_{h}^{t} \in \mathcal{R}^{d_{q} \times d_{t}}$$是可训练参数。 最后，我们学习新闻$$n^u$$和$$n^c$$的基于语义的表示$$\mathbf{t}^{u} \in \mathcal{R}^{d_{t}}$$和$$\mathbf{t}^{c} \in \mathcal{R}^{d_{t}}$$：
$$
\mathbf{t}^{u}=\sum_{i=1}^{M} \beta_{i}^{u} \mathbf{h}_{i}^{u}, \quad \beta_{i}^{u}=\frac{\exp \left(b_{i}^{u}\right)}{\sum_{j=1}^{M} \exp \left(b_{j}^{u}\right)}\\ \tag {15}
$$

$$
\mathbf{t}^{c}=\sum_{i=1}^{M} \beta_{i}^{c} \mathbf{h}_{i}^{c}, \quad \beta_{i}^{c}=\frac{\exp \left(b_{i}^{c}\right)}{\sum_{j=1}^{M} \exp \left(b_{j}^{c}\right)}\tag{16}
$$

​	其中$$\beta_{i}^{u}$$和$$\beta_{i}^{c}$$是文本$$T^u$$和$$T^c$$中第 𝑖 个词的权重。

### 3.4 用户-新闻 协同编码器

​	我们介绍了我们提出的用户新闻协同编码器，它从用户点击的新闻和候选新闻的表示中学习候选新闻感知用户兴趣表示和用户感知候选新闻表示。通常，用户的兴趣是多样的，只有一部分可以与候选新闻匹配[20]。因此，学习候选新闻感知用户兴趣表示可以更好地建模用户兴趣以匹配候选新闻。类似地，候选新闻可能涵盖多个方面，用户可能只对其中的一部分感兴趣 [33, 34]。因此，学习用户感知的候选新闻表示也有利于兴趣匹配。因此，我们应用新闻协同注意网络来学习候选新闻感知用户表示和用户感知候选新闻表示。更具体地说，我们首先根据用户点击新闻$$\mathbf{N}_{u}=\left\{\mathbf{n}_{i}^{u}\right\}_{i=1}^{N} \in R^{d_{n} \times N}$$和候选新闻$$\mathbf{N}_{c}=\left\{\mathbf{n}_{i}^{c}\right\}_{i=1}^{N} \in R^{d_{n} \times N}$$的表示来计算亲和矩阵$$\mathrm{C}_{n} \in R^{N \times N}$$来衡量它们的相关性：
$$
\mathbf{C}_{n}=\mathbf{N}_{c}^{T} \mathbf{W}_{c}^{n} \mathbf{N}_{u}\tag{17}
$$
​	其中𝑁表示点击新闻的数量，$$\mathbf{n}_{i}^{u} \in \mathcal{R}^{d_{n}}$$表示用户第𝑖个点击新闻的表示，$$\mathbf{n}_{i}^{c} \in \mathcal{R}^{d_{n}}$$表示候选新闻的相应表示，$$\mathbf{W}_{c}^{n} \in \mathcal{R}^{d_{n} \times d_{n}}$$是可训练的权重。 然后我们根据亲和度矩阵计算用户点击新闻和候选新闻的表示形式的注意力向量$$\mathbf{r}^{u} \in R^{N}$$和$$\mathbf{r}^{c} \in R^{N}$$：
$$
\mathbf{r}^{u}=\mathbf{q}_{n}^{T} \cdot \tanh \left(\mathbf{W}_{s}^{n} \mathbf{N}_{u}+\mathbf{W}_{h}^{n} \mathbf{N}_{c} f\left(\mathbf{C}_{n}\right)\right) \\ \tag {18}
$$

$$
\mathbf{r}^{c}=\mathbf{q}_{n}^{T} \cdot \tanh \left(\mathbf{W}_{s}^{n} \mathbf{N}_{c}+\mathbf{W}_{h}^{n} \mathbf{N}_{u} f\left(\mathbf{C}_{n}^{T}\right)\right)\tag{19}
$$

​	其中$$\mathbf{q}_{n} \in R^{d_{q}}$$表示可训练的注意力查询，$$\mathbf{W}_{s}^{n} \in\mathcal{R}^{d_{q} \times d_{n}}$$和$$ \mathbf{W}_{h}^{n} \in \mathcal{R}^{d_{q} \times d_{n}}$$是可训练的权重。 候选新闻感知用户表示$$\mathbf{u} \in \mathcal{R}^{d_{n}}$$和用户感知候选新闻表示$$\mathbf{c} \in \mathcal{R}^{d_{n}}$$公式化为：
$$
\mathbf{u}=\sum_{i=1}^{N} \gamma_{i}^{u} \mathbf{n}_{i}^{u}, \quad \gamma_{i}^{u}=\frac{\exp \left(r_{i}^{u}\right)}{\sum_{j=1}^{N} \exp \left(r_{j}^{u}\right)} \\ \tag{20}
$$

$$
\mathbf{c}=\sum_{i=1}^{N} \gamma_{i}^{c} \mathbf{n}_{i}^{c}, \quad \gamma_{i}^{c}=\frac{\exp \left(r_{i}^{c}\right)}{\sum_{j=1}^{N} \exp \left(r_{j}^{c}\right)} \tag{21}
$$

​	其中$$\gamma_{i}^{u}$$和$$\gamma_{i}^{c}$$分别表示$$n_i^u$$和$$n_i^c$$的注意力权重。

### 3.5 关联建模与模型训练

​	跟随 Okura 等人 [21]，我们采用候选新闻感知用户表示u和用户感知候选新闻表示c的点积来衡量用户兴趣和候选新闻内容的相关性𝑧∈R，即$$z=\mathbf{u}^{T} \cdot \mathbf{c}$$。 根据用户的相关性分数，进一步向用户推荐候选新闻。

​	接下来，我们介绍我们如何训练 KIM 方法。 我们利用负采样技术 [7, 9] 构建训练数据集 S，其中每个正样本与从同一新闻印象中随机选择的负样本相关联。 然后，我们应用 NCE 损失 [22] 来制定损失函数：
$$
\mathcal{L}=-\frac{1}{|\mathcal{S}|} \sum_{i=1}^{|\mathcal{S}|} \log \left(\frac{\exp \left(z_{+}^{i}\right)}{\exp \left(z_{+}^{i}\right)+\sum_{j=1}^{U} \exp \left(z_{j}^{i}\right)}\right)\tag{22}
$$
​	其中𝜎表示sigmoid函数，$$z_+^i$$表示第𝑖正样本的相关性分数，$$z_j^i$$表示为第𝑖正样本选择的第𝑗负样本的相关性分数。

​	最后，我们简要讨论 KIM 的计算复杂度。 与单独为用户和候选新闻建模的方法不同，KIM 协同计算点击新闻和候选新闻的表示，这需要更多的计算资源，因为这些表示无法提前准备好。 幸运的是，在实践中，我们可以离线计算不同新闻的上下文词嵌入 H 和实体嵌入 M 并缓存它们以节省计算成本。

## 4 实验

### 4.1 数据集和实验设置

​	我们评估了公开数据集 MIND2 [45] 上不同方法的性能，以及根据 Microsoft 中商业 Feed 应用程序的用户日志构建的私有数据集（名为 Feeds）。 MIND 数据集是根据 在2019年10月12日至11月22日期间的六周内在Microsoft News中采样用户日志构建的，由第五周用户日志构建训练和验证集，由第六周用户日志构建测试集。在 MIND 数据集中，新闻标题中的实体被自动提取并链接到 WikiData。他们的嵌入是基于通过 TransE 方法 [3] 从 WikiData 中提取的知识元组进行训练的。 Feeds 数据集基于 2020 年 1 月 23 日至 4 月 1 日期间的 13 周用户日志构建，其中训练集和验证集分别由前十周随机抽样的 100,000 和 10,000 次展示构建而成，测试集由从过去三周随机抽取的 100,000 次展示。继吴等人之后。 [45]，在 Feeds 数据集中，我们还提取了新闻标题中的实体，并基于 WikiData 预训练了它们的嵌入。在这两个数据集中，我们使用新闻标题作为新闻文本。此外，我们在实验中使用了 WikiData 作为知识图谱。表1列出了更详细的统计数据。

​	接下来，我们介绍 KIM4 的超参数和实验设置。对于每条新闻，我们只使用新闻标题中的前 30 个单词和新闻中的前 5 个实体。我们从知识图中为每个实体随机抽取 10 个邻居。此外，我们只使用了每个用户最近点击的 50 条新闻。词和实体嵌入向量分别由 300 维 glove嵌入 [23] 和 100 维 TransE 嵌入 [3] 初始化。由于 GPU 内存的限制，我们在实验中只微调了词嵌入，没有微调实体嵌入。 Insemantic co-encoder，transformer 包含 10 个注意力头，每个头的输出向量是 40 维。此外，CNN 网络包含 400 个过滤器。在知识协同编码器中，图注意力和协同注意力网络中的所有多头自注意力网络都包含 5 个注意力头，所有这些头都输出 20 维向量。此外，KIM 中的所有注意力查询都设置为 100 维。为了进行有效的模型训练，我们应用了 dropout 技术 [26]，dropout 概率为 0.2。我们为每个正样本抽取了 4 个负样本。我们使用 Adam 优化器 [12] 以 5 × 10−5 的学习率训练 KIM。基于验证数据集选择 KIM 和其他基线方法的所有超参数。根据之前的工作 [33]，我们使用 AUC、MRR、nDCG5 和 nDCG10 进行评估。

### 4.2 性能评估

​	我们将 KIM 与几种最先进的个性化新闻推荐方法进行比较，如下所示： (1) EBNR [21]：通过 GRU 网络从用户的点击历史中表示用户兴趣。 (2) DKN [32]：将多通道 CNN 网络 [15] 应用于新闻标题中对齐的单词和实体的嵌入，以学习新闻表示。 (3) DAN [47]：通过 CNN 网络从新闻标题的单词和实体中学习新闻表示，并通过细心的 LSTM 网络 [8] 学习用户兴趣表示。 (4) NAML [33]：通过多个注意力集中的 CNN 网络从新闻标题、正文、类别和子类别中学习新闻表示。 (5) NPA [34]：使用具有个性化注意查询的注意网络来学习新闻和用户表示。 (6) LSTUR [1]：通过 GRU 网络从用户最近点击的新闻中建模短期用户兴趣，并通过用户 ID 嵌入建模长期用户兴趣。 (7) NRMS [37]：通过多头自注意力网络对新闻内容和用户点击行为进行建模。 (8) KRED [18]：通过图注意力网络从新闻中的实体及其在知识图中的邻居中学习新闻的表示。 (9) FIM [31]：通过CNN网络从用户点击新闻和候选新闻的文本中匹配用户和新闻。

​	我们重复了五次不同的实验，并在表 2 中列出了不同方法的平均性能和相应的标准差。首先，我们可以发现 KIM 明显优于其他基准方法，LSTUR、NRMS和KRED这些方法独立地对候选新闻和用户兴趣进行建模，而不考虑它们的相关性。这是因为用户可能对多个领域感兴趣，并且候选新闻也可能包含多个方面和实体。因此，这些方法很难准确匹配用户兴趣和候选新闻，因为它们是在这些方法中独立建模的。与这些方法不同，在我们的 KIM 方法中，我们提出了一个知识感知交互式匹配框架来交互式地建模用户兴趣和候选新闻。我们的 KIM 可以在语义和知识层面有效地结合点击新闻和候选新闻之间的相关性，以实现更好的兴趣匹配。其次，KIM 还优于通过考虑候选新闻来建模用户兴趣的基线方法，例如 DKN、DAN。这是因为候选新闻可能涵盖多个方面，而用户可能只对其中的一部分感兴趣[33、34]。然而，这些方法在没有考虑目标用户的情况下对候选新闻进行建模，这对于进一步将候选新闻与用户兴趣匹配可能较差。与这些方法不同，我们的 KIM 可以使用目标用户信息对候选新闻进行建模。此外，在这些方法中，点击新闻和候选新闻也是从它们的内容独立建模而不考虑它们的相关性，这对于进一步测量与从点击新闻推断的候选新闻和用户兴趣的相关性可能不是最优的。与这些方法不同，在我们的 KIM 中，我们提出了一个知识协同编码器和一个语义协同编码器，以交互方式学习点击新闻和候选新闻的知识感知表示。

### 4.3 消融实验

​	在本节中，我们进行了两项消融研究来评估 KIM 的有效性。我们首先评估不同信息（即文本和知识）对新闻内容建模的有效性。限于篇幅，以下部分仅展示 MIND 上的实验结果。实验结果如图 6 所示，从中我们有几个观察结果。首先，删除语义信息（即新闻文本）会严重损害 KIM 的性能。这是因为文本通常包含有关新闻内容的丰富信息，对于新闻内容的理解至关重要 [45]。去除语义信息会使新闻表示失去很多重要信息，并且不能准确地对新闻内容进行建模。其次，在新闻内容建模中去除知识图谱（即知识知识图谱中的实体及其邻居）也会使 KIM 的性能显著下降。这是因为文本信息通常不足以理解新闻内容 [18, 32]。幸运的是，知识图谱包含不同实体之间的丰富关联。此外，用户点击新闻中的实体与候选新闻之间的相关性可以提供语义信息之外的丰富信息，以了解用户对候选新闻的兴趣。因此，将实体信息纳入个性化新闻推荐有可能提高推荐的准确性。

​	接下来，我们通过分别用注意力网络替换它们来评估 KIM 中几个重要的注意力网络的有效性。图 7 显示了实验结果，从中我们有几个发现。首先，在用户新闻协同编码器中去除新闻关注网络后，KIM 的性能变得更差。这是因为用户兴趣可能是多样的，并且只有一部分用户点击的新闻对于建模用户兴趣和候选新闻之间的相关性是有用的[32]。此外，候选新闻内容可能包含多个方面，用户可能只对其中的一部分感兴趣。因此，通过新闻共同关注网络学习候选新闻感知用户兴趣和用户感知候选新闻表示可以更好地捕捉用户对候选新闻的兴趣。其次，去除语义共同注意网络也会损害 KIM 的性能。这是因为点击新闻和候选新闻之间的语义相关性可以帮助理解用户对候选新闻的兴趣。此外，一条候选新闻或一条点击新闻通常包含多个方面，其中只有一部分对兴趣匹配有用。如果它们的语义信息是独立建模的，则很难在语义级别有效地捕捉点击新闻和候选新闻的相关性。因此，通过语义共同注意网络交互式学习点击新闻和候选新闻的基于语义的表示可以更好地捕捉它们之间的相关性，以便将用户兴趣与候选新闻匹配。第三，同时去除图协同注意力网络和实体协同注意力网络会导致 KIM 的性能下降。这是因为实体级别的点击新闻和候选新闻之间的相关性对于兴趣匹配也非常有用。此外，如果该方法独立地表示来自其实体的点击新闻和候选新闻，则对于兴趣匹配也是次优的。在KIM方法中，图协同注意力网络和实体协同注意力网络都用于以交互方式捕捉点击新闻和候选新闻实体之间的相关性，可以将丰富的信息融入KIM模型进行兴趣匹配。

### 4.4 知识建模的有效性

​	我们通过比较 KIM 及其变体来评估 KIM 中知识协同编码器的有效性，这些变体独立地对来自其实体的点击和候选新闻进行建模。 第一个是Average，它将新闻中实体的嵌入及其在𝐾跃点内的邻居平均作为基于知识的新闻表示。 第二个是 KCNN，它通过 DKN [32] 中提出的 KCNN 网络从实体及其邻居那里学习基于知识的新闻表示。 第三个是 KGAT，它使用 KRED [18] 中提出的知识图注意力网络从新闻中的实体及其在知识图谱上的邻居中学习基于知识的新闻表示。 此外，所有这些变体都具有与 KIM 相同的文本建模方法，以便进行公平比较。 图 8 显示了实验结果。

​	首先，Average 在这些方法中性能最差。这是因为新闻中的不同实体及其邻居通常对新闻内容理解具有不同的信息量。由于 Average 忽略了不同实体的相对重要性，因此无法有效地基于实体对新闻内容进行建模。其次，KGAT 优于 KCNN。这是因为实体的不同邻居之间通常存在概念上的相关性。KCNN 仅使用新闻中实体邻居的平均嵌入来增强它们的表示，而忽略了这种相关性。与 DKN 不同，KGAT 利用图注意力网络对相邻实体之间的相关性进行建模，可以学习更准确的实体表示。第三，KIM 明显优于所有基线方法，即 Avg、KCNN、KGAT。这是因为实体级别的点击新闻和候选新闻之间的相关性可以提供丰富的线索来推断用户对候选新闻的兴趣。此外，点击新闻或候选新闻可能包含多个实体，并且并非所有实体都可用于将用户兴趣与候选新闻匹配。然而，这些方法独立地对点击新闻和候选新闻的实体信息进行建模，而不考虑它们的相关性，这对于进一步将候选新闻与从点击历史中推断出的用户兴趣进行匹配是次优的。与这些方法不同，我们提出了一种知识协同编码器，可以从实体之间的相关性中交互式地学习基于知识的点击新闻和候选新闻的表示，以实现更好的兴趣匹配。

### 4.5 超参数的影响

​	我们评估了一个重要的超参数，即图共同注意网络的层数，即𝐾，对 KIM 性能的影响。结果如图 9 所示，从中我们有两个观察结果。首先，KIM 的性能首先随着 𝐾 的增加而增加。这是因为点击新闻和候选新闻中的实体之间的相关性有助于了解用户对候选新闻的兴趣。此外，为 𝐾 层堆叠的 GCAT 网络可以在 𝐾 跃点内合并点击新闻和候选新闻中实体的邻居，以学习它们的表示。当𝐾太小时，用户点击的新闻和候选新闻之间的相关性无法根据实体进行充分挖掘，不利于推荐的准确性。其次，当 𝐾 太大时，KIM 的性能开始下降。这是因为当𝐾变得太大时，在对用户点击的新闻和候选新闻之间的相关性建模时考虑了太多的多跳邻居。这可能会给 KIM 模型带来很大的噪音并损害推荐的准确性。因此，适中的 𝐾 值，即 1，适用于 KIM。

### 4.6 案例研究

​	我们进行了一个案例研究，通过将其与 LSTUR 和 KRED 进行比较来展示 KIM 的有效性。我们比较了 LSTUR，因为它在从痛苦新闻文本中模拟新闻内容的基线方法中实现了最佳性能（表 2）。此外，我们比较了 KRED，因为它在知识感知基线方法中实现了最佳性能（表 2）。我们展示了一个随机抽样用户的阅读历史，以及这些方法推荐的新闻在同一印象中，用户只点击了图 10 中的一个候选新闻，从中我们有几个观察结果。首先，KRED 和 KIM 都将用户点击的候选新闻排名高于 LSTUR。这是因为从用户点击新闻和候选新闻的文本信息中很难理解用户兴趣和候选新闻的相关性。但是，由于麦莉·克鲁斯是乡村音乐的代表歌手，在知识图谱上我们可以发现，用户第一次点击新闻中的实体“乡村音乐”与点击的候选新闻中的实体“麦莉·克鲁斯”有一个链接由用户。因此，基于知识图谱提供的信息，KRED 和 KIM 可以更好地了解用户兴趣和候选新闻的相关性。其次，KIM 将用户点击的候选新闻排名高于 KRED。这是因为这两个实体都与知识图谱上的许多其他相邻实体具有丰富的相关性。例如，除了“麦莉·赛勒斯”之外，“乡村音乐”这个实体还与“鲍勃·迪伦”、“泰勒·斯威夫特”等许多其他代表性歌手有关联。此外，“麦莉·克鲁斯”实体还与“麦莉·克鲁斯”擅长的其他领域的实体有关联，如“摇滚音乐”、“流行舞曲”等。然而，独立对用户点击新闻和候选新闻进行建模的KRED难以准确捕捉点击新闻实体和候选新闻实体之间的有用相关性以进行兴趣匹配。与 KRED 不同的是，KIM 使用知识协同编码器从实体级别的相关性中交互式地表示点击新闻和候选新闻，这比 KRED 更能捕捉用户对候选新闻的兴趣。

## 5 结论

​	在本文中，我们提出了一种用于个性化新闻推荐的知识感知交互式匹配框架（命名为 KIM）。该框架旨在对候选新闻和用户兴趣进行交互建模，以实现更准确的兴趣匹配。更具体地说，我们首先提出了一个图协同注意网络，通过选择和聚合它们的邻居的信息来基于知识图对实体进行建模，这些信息为兴趣匹配提供了丰富的信息。我们还提出使用实体协同注意网络从实体之间的相关性中交互式地对点击新闻和候选新闻进行建模。此外，我们建议使用语义共同注意网络从文本之间的语义相关性中交互式地对点击新闻和候选新闻进行建模。此外，我们提出了一种用户新闻协同编码器来学习候选新闻感知用户表示和用户感知候选新闻表示，以更好地捕捉用户兴趣和候选新闻之间的相关性。我们对两个真实世界的数据集进行了广泛的实验。实验结果表明，我们的 KIM 方法显著优于其他基准方法。
